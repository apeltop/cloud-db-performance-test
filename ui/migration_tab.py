"""
Migration tab component for data migration monitoring
Reads statistics from JSON files generated by CLI migration script
"""
import streamlit as st
import pandas as pd
import time
import subprocess
import os
import json
from pathlib import Path
from services.migration.stats_writer import StatsWriter


def render_migration_tab():
    """Render data migration tab with JSON-based monitoring"""
    st.header("🔄 데이터 마이그레이션")
    st.markdown("기존 입찰 데이터를 PostgreSQL 테이블에 마이그레이션합니다.")

    # Initialize stats reader
    stats_writer = StatsWriter()

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("📁 데이터 파일 현황")

        # 데이터 디렉토리 확인
        data_path = Path("data")
        if data_path.exists():
            json_files = [f for f in data_path.glob("*.json") if f.name != "sample_data.json"]

            if json_files:
                st.success(f"✅ {len(json_files)}개의 데이터 파일을 발견했습니다.")

                # 파일 목록 표시
                file_info = []
                total_size = 0

                for file_path in sorted(json_files):
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    total_size += size_mb

                    table_name = "Unknown"
                    if file_path.name.startswith("BidPublicInfoService_BID_CNSTWK_"):
                        table_name = "bid_pblanclistinfo_cnstwk"
                    elif file_path.name.startswith("BidPublicInfoService_BID_SERVC_"):
                        table_name = "bid_pblanclistinfo_servc"
                    elif file_path.name.startswith("BidPublicInfoService_BID_THNG_"):
                        table_name = "bid_pblanclistinfo_thng"
                    elif file_path.name.startswith("BidPublicInfoService_BID_FRGCPT_"):
                        table_name = "bid_pblanclistinfo_frgcpt"
                    elif file_path.name.startswith("PubDataOpnStdService_ScsBidInfo_"):
                        table_name = "opn_std_scsbid_info"

                    file_info.append({
                        "파일명": file_path.name,
                        "크기 (MB)": f"{size_mb:.2f}",
                        "대상 테이블": table_name
                    })

                df_files = pd.DataFrame(file_info)
                st.dataframe(df_files)

                st.info(f"총 데이터 크기: {total_size:.2f} MB")

                # CLI 실행 안내
                st.markdown("---")
                st.subheader("🚀 마이그레이션 실행")

                st.markdown("""
                마이그레이션을 실행하려면 터미널에서 다음 명령어를 실행하세요:

                **기본 실행 (배치 1000개, 단일 커넥션):**
                ```bash
                python migrate_cli.py
                ```

                **배치 크기 변경 (100개):**
                ```bash
                python migrate_cli.py --batch-size 100
                ```

                **멀티 커넥션 (10개):**
                ```bash
                python migrate_cli.py --connections 10
                ```

                **배치 100개 + 커넥션 10개:**
                ```bash
                python migrate_cli.py --batch-size 100 --connections 10
                ```

                **옵션:**
                - `--batch-size`: 배치 크기 (100, 500, 1000, 2000, 5000)
                - `--connections`: 동시 커넥션 수 (1, 2, 5, 10)
                - `--output-dir`: 통계 저장 디렉토리
                """)

                # 간편 실행 버튼 (선택사항)
                if st.button("🚀 여기서 마이그레이션 시작 (백그라운드)"):
                    try:
                        # Start migration in background
                        venv_python = Path("venv/bin/python")
                        if venv_python.exists():
                            python_cmd = str(venv_python)
                        else:
                            python_cmd = "python"

                        process = subprocess.Popen(
                            [python_cmd, "migrate_cli.py"],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            text=True
                        )
                        st.success(f"✅ 마이그레이션 프로세스 시작됨 (PID: {process.pid})")
                        st.info("📊 아래에서 실시간 진행 상황을 모니터링할 수 있습니다.")

                        # Store process info in session state
                        if 'migration_process' not in st.session_state:
                            st.session_state.migration_process = process

                    except Exception as e:
                        st.error(f"❌ 마이그레이션 시작 실패: {e}")

            else:
                st.warning("데이터 파일을 찾을 수 없습니다.")
        else:
            st.error("data 디렉토리를 찾을 수 없습니다.")

    with col2:
        st.subheader("📋 현재 상태")

        # Auto-refresh button
        if st.button("🔄 상태 새로고침"):
            st.experimental_rerun()

        # Auto-refresh toggle
        auto_refresh = st.checkbox("자동 새로고침 (5초마다)", value=False)

        if auto_refresh:
            time.sleep(5)
            st.experimental_rerun()

    # 마이그레이션 진행 상황 모니터링
    st.markdown("---")
    st.subheader("📊 마이그레이션 모니터링")

    # Read progress from JSON
    progress = stats_writer.read_progress()
    stats = stats_writer.read_stats()
    results = stats_writer.read_results()

    # Display current status
    status = progress.get('status', 'idle')

    if status == 'idle':
        st.info("💤 마이그레이션이 시작되지 않았습니다.")
    elif status == 'running':
        st.success("🏃 마이그레이션 진행 중...")

        # Configuration info
        batch_size = progress.get('batch_size', 'N/A')
        num_connections = progress.get('num_connections', 'N/A')

        col_config1, col_config2 = st.columns(2)
        with col_config1:
            st.info(f"⚙️ **배치 크기:** {batch_size}")
        with col_config2:
            st.info(f"🔗 **커넥션 수:** {num_connections}")

        # Progress metrics
        col_a, col_b, col_c = st.columns(3)
        with col_a:
            files_completed = progress.get('files_completed', 0)
            total_files = progress.get('total_files', 0)
            st.metric("파일 진행률", f"{files_completed}/{total_files}")
        with col_b:
            total_records = progress.get('total_records_processed', 0)
            st.metric("처리된 레코드", f"{total_records:,}")
        with col_c:
            current_batch = progress.get('current_batch', 0)
            st.metric("현재 배치", current_batch)

        # Progress bar
        if total_files > 0:
            progress_pct = files_completed / total_files
            st.progress(progress_pct)
            st.caption(f"전체 진행률: {progress_pct*100:.1f}%")

        # Current file
        current_file = progress.get('current_file', '')
        if current_file:
            st.info(f"📂 현재 처리 중: **{current_file}**")

        # Last update time
        last_update = progress.get('last_update')
        if last_update:
            st.caption(f"마지막 업데이트: {last_update}")

        # Real-time batch statistics
        batches = stats.get('batches', [])
        if batches:
            st.markdown("---")
            st.subheader("📈 실시간 배치 통계")

            # Latest batch stats
            latest_batch = batches[-1]
            col_1, col_2, col_3, col_4 = st.columns(4)
            with col_1:
                st.metric("총 배치", len(batches))
            with col_2:
                st.metric("최근 처리량", f"{latest_batch.get('records_per_second', 0):.1f} rec/s")
            with col_3:
                st.metric("누적 레코드", f"{latest_batch.get('cumulative_records', 0):,}")
            with col_4:
                # Average of last 5 batches
                recent_batches = batches[-5:]
                avg_rps = sum(b.get('records_per_second', 0) for b in recent_batches) / len(recent_batches)
                st.metric("평균 처리량 (최근 5배치)", f"{avg_rps:.1f} rec/s")

            # Batch performance chart
            if len(batches) > 1:
                df_batches = pd.DataFrame(batches)

                # Throughput chart
                st.markdown("##### 처리량 추이")
                st.line_chart(
                    df_batches.set_index('batch_number')['records_per_second']
                )

    elif status == 'completed':
        st.success("✅ 마이그레이션 완료!")

        if results:
            # Configuration summary
            batch_size = results.get('batch_size', 'N/A')
            num_connections = results.get('num_connections', 'N/A')

            col_cfg1, col_cfg2 = st.columns(2)
            with col_cfg1:
                st.metric("배치 크기", batch_size)
            with col_cfg2:
                st.metric("커넥션 수", num_connections)

            st.markdown("---")

            # Summary metrics
            col_a, col_b, col_c, col_d = st.columns(4)
            with col_a:
                st.metric("총 파일", results.get('total_files', 0))
            with col_b:
                st.metric("성공", results.get('successful', 0))
            with col_c:
                st.metric("실패", results.get('failed', 0))
            with col_d:
                st.metric("총 레코드", f"{results.get('total_records', 0):,}")

            # Performance metrics
            total_duration = results.get('total_duration_seconds', 0)
            avg_rps = results.get('average_records_per_second', 0)

            col_1, col_2 = st.columns(2)
            with col_1:
                st.metric("총 소요 시간", f"{total_duration:.2f}초")
            with col_2:
                st.metric("평균 처리량", f"{avg_rps:.1f} rec/s")

            # Table counts comparison
            st.markdown("---")
            st.subheader("📊 테이블 레코드 수 변화")

            initial_counts = results.get('initial_counts', {})
            final_counts = results.get('table_counts', {})

            table_comparison = []
            for table in final_counts.keys():
                initial = initial_counts.get(table, 0)
                final = final_counts.get(table, 0)
                added = final - initial
                table_comparison.append({
                    "테이블": table,
                    "초기": f"{initial:,}",
                    "최종": f"{final:,}",
                    "추가": f"{added:,}"
                })

            if table_comparison:
                df_comparison = pd.DataFrame(table_comparison)
                st.dataframe(df_comparison)

            # Batch statistics chart
            batches = stats.get('batches', [])
            if batches and len(batches) > 1:
                st.markdown("---")
                st.subheader("📈 배치별 성능 분석")

                df_batches = pd.DataFrame(batches)

                # Performance charts
                col_chart1, col_chart2 = st.columns(2)

                with col_chart1:
                    st.markdown("##### 배치별 처리 시간")
                    st.line_chart(
                        df_batches.set_index('batch_number')['total_duration_seconds']
                    )

                with col_chart2:
                    st.markdown("##### 배치별 처리량")
                    st.line_chart(
                        df_batches.set_index('batch_number')['records_per_second']
                    )

                # Table-wise statistics
                if 'table_name' in df_batches.columns:
                    st.markdown("##### 테이블별 성능")
                    table_stats = df_batches.groupby('table_name').agg({
                        'records_count': 'sum',
                        'total_duration_seconds': 'sum',
                        'records_per_second': 'mean'
                    }).reset_index()
                    table_stats.columns = ['테이블', '총 레코드', '총 시간 (초)', '평균 처리량 (rec/s)']
                    table_stats['총 레코드'] = table_stats['총 레코드'].apply(lambda x: f"{x:,}")
                    table_stats['총 시간 (초)'] = table_stats['총 시간 (초)'].apply(lambda x: f"{x:.2f}")
                    table_stats['평균 처리량 (rec/s)'] = table_stats['평균 처리량 (rec/s)'].apply(lambda x: f"{x:.1f}")
                    st.dataframe(table_stats)

            # File results
            file_results = results.get('file_results', [])
            if file_results:
                st.markdown("---")
                st.subheader("📁 파일별 결과")

                df_file_results = pd.DataFrame(file_results)
                # Select and rename columns
                display_cols = ['filename', 'table', 'status', 'records_inserted']
                df_display = df_file_results[display_cols].copy()
                df_display.columns = ['파일명', '테이블', '상태', '삽입된 레코드']
                df_display['상태'] = df_display['상태'].map({
                    'success': '✅ 성공',
                    'error': '❌ 실패',
                    'skipped': '⚠️ 건너뜀'
                })
                st.dataframe(df_display)

            # Clear button
            if st.button("🗑️ 통계 초기화"):
                stats_writer.clear_all()
                st.success("통계가 초기화되었습니다.")
                st.experimental_rerun()

    elif status == 'error':
        st.error("❌ 마이그레이션 중 오류 발생")
        error_msg = progress.get('error_message', 'Unknown error')
        st.error(f"오류 메시지: {error_msg}")

        # Clear button
        if st.button("🔄 상태 초기화"):
            stats_writer.clear_all()
            st.experimental_rerun()

    # Historical comparison section
    st.markdown("---")
    st.subheader("📊 성능 비교 분석")

    # Check for historical results
    output_dir = Path("migration_outputs")
    if output_dir.exists():
        results_file = output_dir / "migration_results.json"

        if results_file.exists():
            try:
                with open(results_file, 'r') as f:
                    current_results = json.load(f)

                if current_results.get('status') == 'completed':
                    st.markdown("##### 현재 실행 설정 성능 요약")

                    # Display configuration and performance
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("배치 크기", current_results.get('batch_size', 'N/A'))
                    with col2:
                        st.metric("커넥션 수", current_results.get('num_connections', 'N/A'))
                    with col3:
                        avg_rps = current_results.get('average_records_per_second', 0)
                        st.metric("평균 처리량", f"{avg_rps:.1f} rec/s")
                    with col4:
                        total_time = current_results.get('total_duration_seconds', 0)
                        st.metric("총 소요 시간", f"{total_time:.2f}초")

                    st.markdown("""
                    **성능 테스트 권장 시나리오:**

                    1. **기본 설정 (baseline)**: `--batch-size 1000 --connections 1`
                    2. **작은 배치**: `--batch-size 100 --connections 1`
                    3. **멀티 커넥션**: `--batch-size 1000 --connections 10`
                    4. **작은 배치 + 멀티 커넥션**: `--batch-size 100 --connections 10`

                    각 설정으로 마이그레이션을 실행하여 최적의 성능을 찾으세요!
                    """)

                    # Tips
                    st.info("""
                    💡 **성능 최적화 팁:**
                    - **배치 크기가 작을수록**: 네트워크 왕복 횟수가 증가하지만, 각 트랜잭션이 빨라집니다
                    - **배치 크기가 클수록**: 네트워크 왕복이 줄지만, 각 트랜잭션이 오래 걸립니다
                    - **멀티 커넥션**: CPU 코어를 활용하여 병렬 처리하지만, DB 부하가 증가합니다
                    - **최적 설정**: 데이터 특성과 네트워크 환경에 따라 다릅니다
                    """)
                else:
                    st.info("완료된 마이그레이션 결과가 없습니다. 마이그레이션을 먼저 실행하세요.")
            except Exception as e:
                st.warning(f"결과 파일을 읽을 수 없습니다: {e}")
        else:
            st.info("아직 실행된 마이그레이션이 없습니다.")