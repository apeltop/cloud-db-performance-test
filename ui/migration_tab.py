"""
Migration tab component for data migration monitoring
Reads statistics from JSON files generated by CLI migration script
"""
import streamlit as st
import pandas as pd
import time
import subprocess
import os
from pathlib import Path
from services.migration.stats_writer import StatsWriter


def render_migration_tab():
    """Render data migration tab with JSON-based monitoring"""
    st.header("🔄 데이터 마이그레이션")
    st.markdown("기존 입찰 데이터를 PostgreSQL 테이블에 마이그레이션합니다.")

    # Initialize stats reader
    stats_writer = StatsWriter()

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("📁 데이터 파일 현황")

        # 데이터 디렉토리 확인
        data_path = Path("data")
        if data_path.exists():
            json_files = [f for f in data_path.glob("*.json") if f.name != "sample_data.json"]

            if json_files:
                st.success(f"✅ {len(json_files)}개의 데이터 파일을 발견했습니다.")

                # 파일 목록 표시
                file_info = []
                total_size = 0

                for file_path in sorted(json_files):
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    total_size += size_mb

                    table_name = "Unknown"
                    if file_path.name.startswith("BidPublicInfoService_BID_CNSTWK_"):
                        table_name = "bid_pblanclistinfo_cnstwk"
                    elif file_path.name.startswith("BidPublicInfoService_BID_SERVC_"):
                        table_name = "bid_pblanclistinfo_servc"
                    elif file_path.name.startswith("BidPublicInfoService_BID_THNG_"):
                        table_name = "bid_pblanclistinfo_thng"
                    elif file_path.name.startswith("BidPublicInfoService_BID_FRGCPT_"):
                        table_name = "bid_pblanclistinfo_frgcpt"
                    elif file_path.name.startswith("PubDataOpnStdService_ScsBidInfo_"):
                        table_name = "opn_std_scsbid_info"

                    file_info.append({
                        "파일명": file_path.name,
                        "크기 (MB)": f"{size_mb:.2f}",
                        "대상 테이블": table_name
                    })

                df_files = pd.DataFrame(file_info)
                st.dataframe(df_files)

                st.info(f"총 데이터 크기: {total_size:.2f} MB")

                # CLI 실행 안내
                st.markdown("---")
                st.subheader("🚀 마이그레이션 실행")

                st.markdown("""
                마이그레이션을 실행하려면 터미널에서 다음 명령어를 실행하세요:

                ```bash
                python migrate_cli.py
                ```

                또는 가상환경을 사용하는 경우:

                ```bash
                source venv/bin/activate
                python migrate_cli.py
                ```
                """)

                # 간편 실행 버튼 (선택사항)
                if st.button("🚀 여기서 마이그레이션 시작 (백그라운드)"):
                    try:
                        # Start migration in background
                        venv_python = Path("venv/bin/python")
                        if venv_python.exists():
                            python_cmd = str(venv_python)
                        else:
                            python_cmd = "python"

                        process = subprocess.Popen(
                            [python_cmd, "migrate_cli.py"],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            text=True
                        )
                        st.success(f"✅ 마이그레이션 프로세스 시작됨 (PID: {process.pid})")
                        st.info("📊 아래에서 실시간 진행 상황을 모니터링할 수 있습니다.")

                        # Store process info in session state
                        if 'migration_process' not in st.session_state:
                            st.session_state.migration_process = process

                    except Exception as e:
                        st.error(f"❌ 마이그레이션 시작 실패: {e}")

            else:
                st.warning("데이터 파일을 찾을 수 없습니다.")
        else:
            st.error("data 디렉토리를 찾을 수 없습니다.")

    with col2:
        st.subheader("📋 현재 상태")

        # Auto-refresh button
        if st.button("🔄 상태 새로고침"):
            st.experimental_rerun()

        # Auto-refresh toggle
        auto_refresh = st.checkbox("자동 새로고침 (5초마다)", value=False)

        if auto_refresh:
            time.sleep(5)
            st.experimental_rerun()

    # 마이그레이션 진행 상황 모니터링
    st.markdown("---")
    st.subheader("📊 마이그레이션 모니터링")

    # Read progress from JSON
    progress = stats_writer.read_progress()
    stats = stats_writer.read_stats()
    results = stats_writer.read_results()

    # Display current status
    status = progress.get('status', 'idle')

    if status == 'idle':
        st.info("💤 마이그레이션이 시작되지 않았습니다.")
    elif status == 'running':
        st.success("🏃 마이그레이션 진행 중...")

        # Progress metrics
        col_a, col_b, col_c = st.columns(3)
        with col_a:
            files_completed = progress.get('files_completed', 0)
            total_files = progress.get('total_files', 0)
            st.metric("파일 진행률", f"{files_completed}/{total_files}")
        with col_b:
            total_records = progress.get('total_records_processed', 0)
            st.metric("처리된 레코드", f"{total_records:,}")
        with col_c:
            current_batch = progress.get('current_batch', 0)
            st.metric("현재 배치", current_batch)

        # Progress bar
        if total_files > 0:
            progress_pct = files_completed / total_files
            st.progress(progress_pct)
            st.caption(f"전체 진행률: {progress_pct*100:.1f}%")

        # Current file
        current_file = progress.get('current_file', '')
        if current_file:
            st.info(f"📂 현재 처리 중: **{current_file}**")

        # Last update time
        last_update = progress.get('last_update')
        if last_update:
            st.caption(f"마지막 업데이트: {last_update}")

        # Real-time batch statistics
        batches = stats.get('batches', [])
        if batches:
            st.markdown("---")
            st.subheader("📈 실시간 배치 통계")

            # Latest batch stats
            latest_batch = batches[-1]
            col_1, col_2, col_3, col_4 = st.columns(4)
            with col_1:
                st.metric("총 배치", len(batches))
            with col_2:
                st.metric("최근 처리량", f"{latest_batch.get('records_per_second', 0):.1f} rec/s")
            with col_3:
                st.metric("누적 레코드", f"{latest_batch.get('cumulative_records', 0):,}")
            with col_4:
                # Average of last 5 batches
                recent_batches = batches[-5:]
                avg_rps = sum(b.get('records_per_second', 0) for b in recent_batches) / len(recent_batches)
                st.metric("평균 처리량 (최근 5배치)", f"{avg_rps:.1f} rec/s")

            # Batch performance chart
            if len(batches) > 1:
                df_batches = pd.DataFrame(batches)

                # Throughput chart
                st.markdown("##### 처리량 추이")
                st.line_chart(
                    df_batches.set_index('batch_number')['records_per_second']
                )

    elif status == 'completed':
        st.success("✅ 마이그레이션 완료!")

        if results:
            # Summary metrics
            col_a, col_b, col_c, col_d = st.columns(4)
            with col_a:
                st.metric("총 파일", results.get('total_files', 0))
            with col_b:
                st.metric("성공", results.get('successful', 0))
            with col_c:
                st.metric("실패", results.get('failed', 0))
            with col_d:
                st.metric("총 레코드", f"{results.get('total_records', 0):,}")

            # Performance metrics
            total_duration = results.get('total_duration_seconds', 0)
            avg_rps = results.get('average_records_per_second', 0)

            col_1, col_2 = st.columns(2)
            with col_1:
                st.metric("총 소요 시간", f"{total_duration:.2f}초")
            with col_2:
                st.metric("평균 처리량", f"{avg_rps:.1f} rec/s")

            # Table counts comparison
            st.markdown("---")
            st.subheader("📊 테이블 레코드 수 변화")

            initial_counts = results.get('initial_counts', {})
            final_counts = results.get('table_counts', {})

            table_comparison = []
            for table in final_counts.keys():
                initial = initial_counts.get(table, 0)
                final = final_counts.get(table, 0)
                added = final - initial
                table_comparison.append({
                    "테이블": table,
                    "초기": f"{initial:,}",
                    "최종": f"{final:,}",
                    "추가": f"{added:,}"
                })

            if table_comparison:
                df_comparison = pd.DataFrame(table_comparison)
                st.dataframe(df_comparison)

            # Batch statistics chart
            batches = stats.get('batches', [])
            if batches and len(batches) > 1:
                st.markdown("---")
                st.subheader("📈 배치별 성능 분석")

                df_batches = pd.DataFrame(batches)

                # Performance charts
                col_chart1, col_chart2 = st.columns(2)

                with col_chart1:
                    st.markdown("##### 배치별 처리 시간")
                    st.line_chart(
                        df_batches.set_index('batch_number')['total_duration_seconds']
                    )

                with col_chart2:
                    st.markdown("##### 배치별 처리량")
                    st.line_chart(
                        df_batches.set_index('batch_number')['records_per_second']
                    )

                # Table-wise statistics
                if 'table_name' in df_batches.columns:
                    st.markdown("##### 테이블별 성능")
                    table_stats = df_batches.groupby('table_name').agg({
                        'records_count': 'sum',
                        'total_duration_seconds': 'sum',
                        'records_per_second': 'mean'
                    }).reset_index()
                    table_stats.columns = ['테이블', '총 레코드', '총 시간 (초)', '평균 처리량 (rec/s)']
                    table_stats['총 레코드'] = table_stats['총 레코드'].apply(lambda x: f"{x:,}")
                    table_stats['총 시간 (초)'] = table_stats['총 시간 (초)'].apply(lambda x: f"{x:.2f}")
                    table_stats['평균 처리량 (rec/s)'] = table_stats['평균 처리량 (rec/s)'].apply(lambda x: f"{x:.1f}")
                    st.dataframe(table_stats)

            # File results
            file_results = results.get('file_results', [])
            if file_results:
                st.markdown("---")
                st.subheader("📁 파일별 결과")

                df_file_results = pd.DataFrame(file_results)
                # Select and rename columns
                display_cols = ['filename', 'table', 'status', 'records_inserted']
                df_display = df_file_results[display_cols].copy()
                df_display.columns = ['파일명', '테이블', '상태', '삽입된 레코드']
                df_display['상태'] = df_display['상태'].map({
                    'success': '✅ 성공',
                    'error': '❌ 실패',
                    'skipped': '⚠️ 건너뜀'
                })
                st.dataframe(df_display)

            # Clear button
            if st.button("🗑️ 통계 초기화"):
                stats_writer.clear_all()
                st.success("통계가 초기화되었습니다.")
                st.experimental_rerun()

    elif status == 'error':
        st.error("❌ 마이그레이션 중 오류 발생")
        error_msg = progress.get('error_message', 'Unknown error')
        st.error(f"오류 메시지: {error_msg}")

        # Clear button
        if st.button("🔄 상태 초기화"):
            stats_writer.clear_all()
            st.experimental_rerun()