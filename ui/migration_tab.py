"""
Migration tab component for data migration monitoring
Reads statistics from JSON files generated by CLI migration script
"""
import streamlit as st
import pandas as pd
import time
import subprocess
import os
import json
from pathlib import Path
from services.migration.stats_writer import StatsWriter


def render_migration_tab():
    """Render data migration tab with JSON-based monitoring"""
    st.header("ğŸ”„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
    st.markdown("ê¸°ì¡´ ì…ì°° ë°ì´í„°ë¥¼ PostgreSQL í…Œì´ë¸”ì— ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.")

    # Initialize stats reader
    stats_writer = StatsWriter()

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("ğŸ“ ë°ì´í„° íŒŒì¼ í˜„í™©")

        # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
        data_path = Path("data")
        if data_path.exists():
            json_files = [f for f in data_path.glob("*.json") if f.name != "sample_data.json"]

            if json_files:
                st.success(f"âœ… {len(json_files)}ê°œì˜ ë°ì´í„° íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

                # íŒŒì¼ ëª©ë¡ í‘œì‹œ
                file_info = []
                total_size = 0

                for file_path in sorted(json_files):
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    total_size += size_mb

                    table_name = "Unknown"
                    if file_path.name.startswith("BidPublicInfoService_BID_CNSTWK_"):
                        table_name = "bid_pblanclistinfo_cnstwk"
                    elif file_path.name.startswith("BidPublicInfoService_BID_SERVC_"):
                        table_name = "bid_pblanclistinfo_servc"
                    elif file_path.name.startswith("BidPublicInfoService_BID_THNG_"):
                        table_name = "bid_pblanclistinfo_thng"
                    elif file_path.name.startswith("BidPublicInfoService_BID_FRGCPT_"):
                        table_name = "bid_pblanclistinfo_frgcpt"
                    elif file_path.name.startswith("PubDataOpnStdService_ScsBidInfo_"):
                        table_name = "opn_std_scsbid_info"

                    file_info.append({
                        "íŒŒì¼ëª…": file_path.name,
                        "í¬ê¸° (MB)": f"{size_mb:.2f}",
                        "ëŒ€ìƒ í…Œì´ë¸”": table_name
                    })

                df_files = pd.DataFrame(file_info)
                st.dataframe(df_files)

                st.info(f"ì´ ë°ì´í„° í¬ê¸°: {total_size:.2f} MB")

                # CLI ì‹¤í–‰ ì•ˆë‚´
                st.markdown("---")
                st.subheader("ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰")

                st.markdown("""
                ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ë ¤ë©´ í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:

                **ê¸°ë³¸ ì‹¤í–‰ (ë°°ì¹˜ 1000ê°œ, ë‹¨ì¼ ì»¤ë„¥ì…˜):**
                ```bash
                python migrate_cli.py
                ```

                **ë°°ì¹˜ í¬ê¸° ë³€ê²½ (100ê°œ):**
                ```bash
                python migrate_cli.py --batch-size 100
                ```

                **ë©€í‹° ì»¤ë„¥ì…˜ (10ê°œ):**
                ```bash
                python migrate_cli.py --connections 10
                ```

                **ë°°ì¹˜ 100ê°œ + ì»¤ë„¥ì…˜ 10ê°œ:**
                ```bash
                python migrate_cli.py --batch-size 100 --connections 10
                ```

                **ì˜µì…˜:**
                - `--batch-size`: ë°°ì¹˜ í¬ê¸° (100, 500, 1000, 2000, 5000)
                - `--connections`: ë™ì‹œ ì»¤ë„¥ì…˜ ìˆ˜ (1, 2, 5, 10)
                - `--output-dir`: í†µê³„ ì €ì¥ ë””ë ‰í† ë¦¬
                """)

                # ê°„í¸ ì‹¤í–‰ ë²„íŠ¼ (ì„ íƒì‚¬í•­)
                if st.button("ğŸš€ ì—¬ê¸°ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)"):
                    try:
                        # Start migration in background
                        venv_python = Path("venv/bin/python")
                        if venv_python.exists():
                            python_cmd = str(venv_python)
                        else:
                            python_cmd = "python"

                        process = subprocess.Popen(
                            [python_cmd, "migrate_cli.py"],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            text=True
                        )
                        st.success(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨ (PID: {process.pid})")
                        st.info("ğŸ“Š ì•„ë˜ì—ì„œ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                        # Store process info in session state
                        if 'migration_process' not in st.session_state:
                            st.session_state.migration_process = process

                    except Exception as e:
                        st.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ ì‹¤íŒ¨: {e}")

            else:
                st.warning("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error("data ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    with col2:
        st.subheader("ğŸ“‹ í˜„ì¬ ìƒíƒœ")

        # Auto-refresh button
        if st.button("ğŸ”„ ìƒíƒœ ìƒˆë¡œê³ ì¹¨"):
            st.experimental_rerun()

        # Auto-refresh toggle
        auto_refresh = st.checkbox("ìë™ ìƒˆë¡œê³ ì¹¨ (5ì´ˆë§ˆë‹¤)", value=False)

        if auto_refresh:
            time.sleep(5)
            st.experimental_rerun()

    # ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
    st.markdown("---")
    st.subheader("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ëª¨ë‹ˆí„°ë§")

    # Read progress from JSON
    progress = stats_writer.read_progress()
    stats = stats_writer.read_stats()
    results = stats_writer.read_results()

    # Display current status
    status = progress.get('status', 'idle')

    if status == 'idle':
        st.info("ğŸ’¤ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    elif status == 'running':
        st.success("ğŸƒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰ ì¤‘...")

        # Configuration info
        batch_size = progress.get('batch_size', 'N/A')
        num_connections = progress.get('num_connections', 'N/A')

        col_config1, col_config2 = st.columns(2)
        with col_config1:
            st.info(f"âš™ï¸ **ë°°ì¹˜ í¬ê¸°:** {batch_size}")
        with col_config2:
            st.info(f"ğŸ”— **ì»¤ë„¥ì…˜ ìˆ˜:** {num_connections}")

        # Progress metrics
        col_a, col_b, col_c = st.columns(3)
        with col_a:
            files_completed = progress.get('files_completed', 0)
            total_files = progress.get('total_files', 0)
            st.metric("íŒŒì¼ ì§„í–‰ë¥ ", f"{files_completed}/{total_files}")
        with col_b:
            total_records = progress.get('total_records_processed', 0)
            st.metric("ì²˜ë¦¬ëœ ë ˆì½”ë“œ", f"{total_records:,}")
        with col_c:
            current_batch = progress.get('current_batch', 0)
            st.metric("í˜„ì¬ ë°°ì¹˜", current_batch)

        # Progress bar
        if total_files > 0:
            progress_pct = files_completed / total_files
            st.progress(progress_pct)
            st.caption(f"ì „ì²´ ì§„í–‰ë¥ : {progress_pct*100:.1f}%")

        # Current file
        current_file = progress.get('current_file', '')
        if current_file:
            st.info(f"ğŸ“‚ í˜„ì¬ ì²˜ë¦¬ ì¤‘: **{current_file}**")

        # Last update time
        last_update = progress.get('last_update')
        if last_update:
            st.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update}")

        # Real-time batch statistics
        batches = stats.get('batches', [])
        if batches:
            st.markdown("---")
            st.subheader("ğŸ“ˆ ì‹¤ì‹œê°„ ë°°ì¹˜ í†µê³„")

            # Latest batch stats
            latest_batch = batches[-1]
            col_1, col_2, col_3, col_4 = st.columns(4)
            with col_1:
                st.metric("ì´ ë°°ì¹˜", len(batches))
            with col_2:
                st.metric("ìµœê·¼ ì²˜ë¦¬ëŸ‰", f"{latest_batch.get('records_per_second', 0):.1f} rec/s")
            with col_3:
                st.metric("ëˆ„ì  ë ˆì½”ë“œ", f"{latest_batch.get('cumulative_records', 0):,}")
            with col_4:
                # Average of last 5 batches
                recent_batches = batches[-5:]
                avg_rps = sum(b.get('records_per_second', 0) for b in recent_batches) / len(recent_batches)
                st.metric("í‰ê·  ì²˜ë¦¬ëŸ‰ (ìµœê·¼ 5ë°°ì¹˜)", f"{avg_rps:.1f} rec/s")

            # Batch performance chart
            if len(batches) > 1:
                df_batches = pd.DataFrame(batches)

                # Throughput chart
                st.markdown("##### ì²˜ë¦¬ëŸ‰ ì¶”ì´")
                st.line_chart(
                    df_batches.set_index('batch_number')['records_per_second']
                )

    elif status == 'completed':
        st.success("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")

        if results:
            # Configuration summary
            batch_size = results.get('batch_size', 'N/A')
            num_connections = results.get('num_connections', 'N/A')

            col_cfg1, col_cfg2 = st.columns(2)
            with col_cfg1:
                st.metric("ë°°ì¹˜ í¬ê¸°", batch_size)
            with col_cfg2:
                st.metric("ì»¤ë„¥ì…˜ ìˆ˜", num_connections)

            st.markdown("---")

            # Summary metrics
            col_a, col_b, col_c, col_d = st.columns(4)
            with col_a:
                st.metric("ì´ íŒŒì¼", results.get('total_files', 0))
            with col_b:
                st.metric("ì„±ê³µ", results.get('successful', 0))
            with col_c:
                st.metric("ì‹¤íŒ¨", results.get('failed', 0))
            with col_d:
                st.metric("ì´ ë ˆì½”ë“œ", f"{results.get('total_records', 0):,}")

            # Performance metrics
            total_duration = results.get('total_duration_seconds', 0)
            avg_rps = results.get('average_records_per_second', 0)

            col_1, col_2 = st.columns(2)
            with col_1:
                st.metric("ì´ ì†Œìš” ì‹œê°„", f"{total_duration:.2f}ì´ˆ")
            with col_2:
                st.metric("í‰ê·  ì²˜ë¦¬ëŸ‰", f"{avg_rps:.1f} rec/s")

            # Table counts comparison
            st.markdown("---")
            st.subheader("ğŸ“Š í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜ ë³€í™”")

            initial_counts = results.get('initial_counts', {})
            final_counts = results.get('table_counts', {})

            table_comparison = []
            for table in final_counts.keys():
                initial = initial_counts.get(table, 0)
                final = final_counts.get(table, 0)
                added = final - initial
                table_comparison.append({
                    "í…Œì´ë¸”": table,
                    "ì´ˆê¸°": f"{initial:,}",
                    "ìµœì¢…": f"{final:,}",
                    "ì¶”ê°€": f"{added:,}"
                })

            if table_comparison:
                df_comparison = pd.DataFrame(table_comparison)
                st.dataframe(df_comparison)

            # Batch statistics chart
            batches = stats.get('batches', [])
            if batches and len(batches) > 1:
                st.markdown("---")
                st.subheader("ğŸ“ˆ ë°°ì¹˜ë³„ ì„±ëŠ¥ ë¶„ì„")

                df_batches = pd.DataFrame(batches)

                # Performance charts
                col_chart1, col_chart2 = st.columns(2)

                with col_chart1:
                    st.markdown("##### ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹œê°„")
                    st.line_chart(
                        df_batches.set_index('batch_number')['total_duration_seconds']
                    )

                with col_chart2:
                    st.markdown("##### ë°°ì¹˜ë³„ ì²˜ë¦¬ëŸ‰")
                    st.line_chart(
                        df_batches.set_index('batch_number')['records_per_second']
                    )

                # Table-wise statistics
                if 'table_name' in df_batches.columns:
                    st.markdown("##### í…Œì´ë¸”ë³„ ì„±ëŠ¥")
                    table_stats = df_batches.groupby('table_name').agg({
                        'records_count': 'sum',
                        'total_duration_seconds': 'sum',
                        'records_per_second': 'mean'
                    }).reset_index()
                    table_stats.columns = ['í…Œì´ë¸”', 'ì´ ë ˆì½”ë“œ', 'ì´ ì‹œê°„ (ì´ˆ)', 'í‰ê·  ì²˜ë¦¬ëŸ‰ (rec/s)']
                    table_stats['ì´ ë ˆì½”ë“œ'] = table_stats['ì´ ë ˆì½”ë“œ'].apply(lambda x: f"{x:,}")
                    table_stats['ì´ ì‹œê°„ (ì´ˆ)'] = table_stats['ì´ ì‹œê°„ (ì´ˆ)'].apply(lambda x: f"{x:.2f}")
                    table_stats['í‰ê·  ì²˜ë¦¬ëŸ‰ (rec/s)'] = table_stats['í‰ê·  ì²˜ë¦¬ëŸ‰ (rec/s)'].apply(lambda x: f"{x:.1f}")
                    st.dataframe(table_stats)

            # File results
            file_results = results.get('file_results', [])
            if file_results:
                st.markdown("---")
                st.subheader("ğŸ“ íŒŒì¼ë³„ ê²°ê³¼")

                df_file_results = pd.DataFrame(file_results)
                # Select and rename columns
                display_cols = ['filename', 'table', 'status', 'records_inserted']
                df_display = df_file_results[display_cols].copy()
                df_display.columns = ['íŒŒì¼ëª…', 'í…Œì´ë¸”', 'ìƒíƒœ', 'ì‚½ì…ëœ ë ˆì½”ë“œ']
                df_display['ìƒíƒœ'] = df_display['ìƒíƒœ'].map({
                    'success': 'âœ… ì„±ê³µ',
                    'error': 'âŒ ì‹¤íŒ¨',
                    'skipped': 'âš ï¸ ê±´ë„ˆëœ€'
                })
                st.dataframe(df_display)

            # Clear button
            if st.button("ğŸ—‘ï¸ í†µê³„ ì´ˆê¸°í™”"):
                stats_writer.clear_all()
                st.success("í†µê³„ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.experimental_rerun()

    elif status == 'error':
        st.error("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        error_msg = progress.get('error_message', 'Unknown error')
        st.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {error_msg}")

        # Clear button
        if st.button("ğŸ”„ ìƒíƒœ ì´ˆê¸°í™”"):
            stats_writer.clear_all()
            st.experimental_rerun()

    # Historical comparison section
    st.markdown("---")
    st.subheader("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„")

    # Check for historical results
    output_dir = Path("migration_outputs")
    if output_dir.exists():
        results_file = output_dir / "migration_results.json"

        if results_file.exists():
            try:
                with open(results_file, 'r') as f:
                    current_results = json.load(f)

                if current_results.get('status') == 'completed':
                    st.markdown("##### í˜„ì¬ ì‹¤í–‰ ì„¤ì • ì„±ëŠ¥ ìš”ì•½")

                    # Display configuration and performance
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ë°°ì¹˜ í¬ê¸°", current_results.get('batch_size', 'N/A'))
                    with col2:
                        st.metric("ì»¤ë„¥ì…˜ ìˆ˜", current_results.get('num_connections', 'N/A'))
                    with col3:
                        avg_rps = current_results.get('average_records_per_second', 0)
                        st.metric("í‰ê·  ì²˜ë¦¬ëŸ‰", f"{avg_rps:.1f} rec/s")
                    with col4:
                        total_time = current_results.get('total_duration_seconds', 0)
                        st.metric("ì´ ì†Œìš” ì‹œê°„", f"{total_time:.2f}ì´ˆ")

                    st.markdown("""
                    **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤:**

                    1. **ê¸°ë³¸ ì„¤ì • (baseline)**: `--batch-size 1000 --connections 1`
                    2. **ì‘ì€ ë°°ì¹˜**: `--batch-size 100 --connections 1`
                    3. **ë©€í‹° ì»¤ë„¥ì…˜**: `--batch-size 1000 --connections 10`
                    4. **ì‘ì€ ë°°ì¹˜ + ë©€í‹° ì»¤ë„¥ì…˜**: `--batch-size 100 --connections 10`

                    ê° ì„¤ì •ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì—¬ ìµœì ì˜ ì„±ëŠ¥ì„ ì°¾ìœ¼ì„¸ìš”!
                    """)

                    # Tips
                    st.info("""
                    ğŸ’¡ **ì„±ëŠ¥ ìµœì í™” íŒ:**
                    - **ë°°ì¹˜ í¬ê¸°ê°€ ì‘ì„ìˆ˜ë¡**: ë„¤íŠ¸ì›Œí¬ ì™•ë³µ íšŸìˆ˜ê°€ ì¦ê°€í•˜ì§€ë§Œ, ê° íŠ¸ëœì­ì…˜ì´ ë¹¨ë¼ì§‘ë‹ˆë‹¤
                    - **ë°°ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡**: ë„¤íŠ¸ì›Œí¬ ì™•ë³µì´ ì¤„ì§€ë§Œ, ê° íŠ¸ëœì­ì…˜ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤
                    - **ë©€í‹° ì»¤ë„¥ì…˜**: CPU ì½”ì–´ë¥¼ í™œìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬í•˜ì§€ë§Œ, DB ë¶€í•˜ê°€ ì¦ê°€í•©ë‹ˆë‹¤
                    - **ìµœì  ì„¤ì •**: ë°ì´í„° íŠ¹ì„±ê³¼ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤
                    """)
                else:
                    st.info("ì™„ë£Œëœ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            except Exception as e:
                st.warning(f"ê²°ê³¼ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        else:
            st.info("ì•„ì§ ì‹¤í–‰ëœ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")