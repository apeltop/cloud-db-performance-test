"""
Comparison tab for comparing multiple test runs
Provides visualization and analysis of performance across different configurations
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
from services.migration.test_run_manager import TestRunManager
from utils.comparison_utils import (
    calculate_performance_metrics,
    analyze_performance_comparison,
    prepare_batch_comparison_data,
    get_test_summary
)


def render_comparison_tab():
    """Render test comparison tab"""
    st.header("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ")

    # Initialize test run manager
    test_manager = TestRunManager()

    # Get all test runs
    all_test_runs = test_manager.get_all_test_runs()

    if not all_test_runs:
        st.info("ì•„ì§ ì €ì¥ëœ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. CLIë¥¼ í†µí•´ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        st.code("python migrate_cli.py --batch-size 1000 --connections 1")
        return

    # Filter options
    st.subheader("ğŸ” í•„í„°")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        # Get unique providers
        providers = list(set([tr.get('cloud_provider', 'Unknown') for tr in all_test_runs]))
        selected_provider = st.selectbox("í´ë¼ìš°ë“œ í”„ë¡œë°”ì´ë”", ["ì „ì²´"] + providers)

    with col2:
        # Get unique batch sizes
        batch_sizes = list(set([tr.get('batch_size', 0) for tr in all_test_runs]))
        selected_batch = st.selectbox("ë°°ì¹˜ í¬ê¸°", ["ì „ì²´"] + sorted(batch_sizes))

    with col3:
        # Get unique connection counts
        connections = list(set([tr.get('num_connections', 0) for tr in all_test_runs]))
        selected_conn = st.selectbox("ì»¤ë„¥ì…˜ ìˆ˜", ["ì „ì²´"] + sorted(connections))

    with col4:
        # Status filter
        selected_status = st.selectbox("ìƒíƒœ", ["ì „ì²´", "completed", "running", "error"])

    # Apply filters
    filtered_runs = all_test_runs
    if selected_provider != "ì „ì²´":
        filtered_runs = [tr for tr in filtered_runs if tr.get('cloud_provider') == selected_provider]
    if selected_batch != "ì „ì²´":
        filtered_runs = [tr for tr in filtered_runs if tr.get('batch_size') == selected_batch]
    if selected_conn != "ì „ì²´":
        filtered_runs = [tr for tr in filtered_runs if tr.get('num_connections') == selected_conn]
    if selected_status != "ì „ì²´":
        filtered_runs = [tr for tr in filtered_runs if tr.get('status') == selected_status]

    st.markdown("---")

    # Display test runs table
    st.subheader("ğŸ“‹ ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ëª©ë¡")

    if not filtered_runs:
        st.warning("í•„í„° ì¡°ê±´ì— ë§ëŠ” í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # Prepare table data
    table_data = []
    for tr in filtered_runs:
        table_data.append({
            'ì„ íƒ': False,
            'í…ŒìŠ¤íŠ¸ ID': tr['test_id'],
            'ì‹œê°„': tr['timestamp'][:19] if tr.get('timestamp') else 'N/A',
            'Provider': tr.get('cloud_provider', 'Unknown'),
            'Instance': tr.get('instance_type', 'Unknown'),
            'Batch': tr.get('batch_size', 0),
            'Conn': tr.get('num_connections', 0),
            'RPS': f"{tr.get('average_records_per_second', 0):.0f}" if tr.get('average_records_per_second') else 'N/A',
            'ìƒíƒœ': 'âœ…' if tr.get('status') == 'completed' else ('âš ï¸' if tr.get('status') == 'running' else 'âŒ')
        })

    df_tests = pd.DataFrame(table_data)

    # Display dataframe with selection
    st.dataframe(df_tests, use_container_width=True, hide_index=True)

    # Multi-select for comparison
    st.markdown("---")
    st.subheader("ğŸ¯ ë¹„êµí•  í…ŒìŠ¤íŠ¸ ì„ íƒ")

    # Create selection options with readable labels
    test_options = {}
    for tr in filtered_runs:
        if tr.get('status') == 'completed':
            label = f"{tr['timestamp'][:19]} - {get_test_summary(tr)} - {tr.get('average_records_per_second', 0):.0f} rec/s"
            test_options[label] = tr['test_id']

    if not test_options:
        st.warning("ë¹„êµí•  ìˆ˜ ìˆëŠ” ì™„ë£Œëœ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    selected_labels = st.multiselect(
        "ë¹„êµí•  í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (2ê°œ ì´ìƒ)",
        options=list(test_options.keys()),
        default=list(test_options.keys())[:min(2, len(test_options))]
    )

    selected_test_ids = [test_options[label] for label in selected_labels]

    if len(selected_test_ids) < 2:
        st.info("ë¹„êµ ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 2ê°œì˜ í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        return

    # Get selected test runs
    selected_tests = [tr for tr in filtered_runs if tr['test_id'] in selected_test_ids]

    # Comparison button
    if st.button("ğŸ“Š ì„ íƒí•œ í…ŒìŠ¤íŠ¸ ë¹„êµ ë¶„ì„", type="primary"):
        render_comparison_analysis(selected_tests, test_manager)


def render_comparison_analysis(selected_tests: list, test_manager: TestRunManager):
    """Render comparison analysis and charts"""
    st.markdown("---")
    st.header("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ë¶„ì„")

    # Performance analysis
    analysis = analyze_performance_comparison(selected_tests)

    if analysis['status'] == 'no_data':
        st.warning(analysis['message'])
        return

    # Summary metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ë¹„êµ í…ŒìŠ¤íŠ¸ ìˆ˜", analysis['total_tests'])
    with col2:
        st.metric("í‰ê·  ì²˜ë¦¬ëŸ‰", f"{analysis['averages']['throughput']:.1f} rec/s")
    with col3:
        st.metric("í‰ê·  ì²˜ë¦¬ ì‹œê°„", f"{analysis['averages']['duration']:.1f}ì´ˆ")

    # Best/Worst performers
    st.markdown("### ğŸ† ì„±ëŠ¥ ìˆœìœ„")
    col1, col2 = st.columns(2)

    with col1:
        st.success("**ìµœê³  ì²˜ë¦¬ëŸ‰**")
        best = analysis['best_throughput']
        st.write(f"**Provider:** {best['provider']}")
        st.write(f"**Instance:** {best['instance']}")
        st.write(f"**ì²˜ë¦¬ëŸ‰:** {best['value']:.1f} rec/s")

    with col2:
        st.success("**ìµœë‹¨ ì²˜ë¦¬ ì‹œê°„**")
        fastest = analysis['fastest_duration']
        st.write(f"**Provider:** {fastest['provider']}")
        st.write(f"**Instance:** {fastest['instance']}")
        st.write(f"**ì‹œê°„:** {fastest['value']:.1f}ì´ˆ")

    st.markdown("---")

    # Comparison charts
    st.markdown("### ğŸ“Š ë¹„êµ ì°¨íŠ¸")

    # Prepare data for charts
    df_metrics = calculate_performance_metrics(selected_tests)

    # Chart 1: Total Duration Comparison
    st.markdown("#### â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„ ë¹„êµ")
    fig_duration = px.bar(
        df_metrics,
        x='test_id',
        y='total_duration_seconds',
        color='cloud_provider',
        title='ì´ ì²˜ë¦¬ ì‹œê°„ ë¹„êµ',
        labels={
            'test_id': 'í…ŒìŠ¤íŠ¸ ID',
            'total_duration_seconds': 'ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)',
            'cloud_provider': 'Provider'
        },
        text='total_duration_seconds'
    )
    fig_duration.update_traces(texttemplate='%{text:.1f}s', textposition='outside')
    fig_duration.update_layout(height=400, xaxis_tickangle=-45)
    st.plotly_chart(fig_duration, use_container_width=True)

    # Chart 2: Throughput Comparison
    st.markdown("#### ğŸš€ í‰ê·  ì²˜ë¦¬ëŸ‰ ë¹„êµ")
    fig_throughput = px.bar(
        df_metrics,
        x='test_id',
        y='average_records_per_second',
        color='cloud_provider',
        title='í‰ê·  ì²˜ë¦¬ëŸ‰ ë¹„êµ',
        labels={
            'test_id': 'í…ŒìŠ¤íŠ¸ ID',
            'average_records_per_second': 'ì²˜ë¦¬ëŸ‰ (records/sec)',
            'cloud_provider': 'Provider'
        },
        text='average_records_per_second'
    )
    fig_throughput.update_traces(texttemplate='%{text:.0f}', textposition='outside')
    fig_throughput.update_layout(height=400, xaxis_tickangle=-45)
    st.plotly_chart(fig_throughput, use_container_width=True)

    # Chart 3: Batch-level performance overlay
    st.markdown("#### ğŸ“ˆ ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹œê°„ ì¶”ì´ (ì˜¤ë²„ë ˆì´)")
    batch_df = prepare_batch_comparison_data(selected_tests, test_manager.base_output_dir)

    if not batch_df.empty:
        fig_batch = px.line(
            batch_df,
            x='batch_number',
            y='total_duration_seconds',
            color='test_label',
            title='ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹œê°„ ì¶”ì´',
            labels={
                'batch_number': 'ë°°ì¹˜ ë²ˆí˜¸',
                'total_duration_seconds': 'ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)',
                'test_label': 'í…ŒìŠ¤íŠ¸'
            },
            markers=True
        )
        fig_batch.update_layout(height=500)
        st.plotly_chart(fig_batch, use_container_width=True)

        # Chart 4: Batch-level throughput overlay
        st.markdown("#### ğŸ“ˆ ë°°ì¹˜ë³„ ì²˜ë¦¬ëŸ‰ ì¶”ì´ (ì˜¤ë²„ë ˆì´)")
        fig_batch_rps = px.line(
            batch_df,
            x='batch_number',
            y='records_per_second',
            color='test_label',
            title='ë°°ì¹˜ë³„ ì²˜ë¦¬ëŸ‰ ì¶”ì´',
            labels={
                'batch_number': 'ë°°ì¹˜ ë²ˆí˜¸',
                'records_per_second': 'ì²˜ë¦¬ëŸ‰ (records/sec)',
                'test_label': 'í…ŒìŠ¤íŠ¸'
            },
            markers=True
        )
        fig_batch_rps.update_layout(height=500)
        st.plotly_chart(fig_batch_rps, use_container_width=True)

        # Chart 5: Time breakdown comparison (stacked bar)
        st.markdown("#### â±ï¸ ì‹œê°„ êµ¬ì„± ë¹„êµ (í‰ê· )")
        time_breakdown = batch_df.groupby('test_label').agg({
            'data_preparation_time': 'mean',
            'query_execution_time': 'mean',
            'commit_time': 'mean',
            'overhead_time': 'mean'
        }).reset_index()

        fig_breakdown = go.Figure()
        fig_breakdown.add_trace(go.Bar(
            name='Data Preparation',
            x=time_breakdown['test_label'],
            y=time_breakdown['data_preparation_time'],
            text=time_breakdown['data_preparation_time'].round(3),
            textposition='inside'
        ))
        fig_breakdown.add_trace(go.Bar(
            name='Query Execution',
            x=time_breakdown['test_label'],
            y=time_breakdown['query_execution_time'],
            text=time_breakdown['query_execution_time'].round(3),
            textposition='inside'
        ))
        fig_breakdown.add_trace(go.Bar(
            name='Commit',
            x=time_breakdown['test_label'],
            y=time_breakdown['commit_time'],
            text=time_breakdown['commit_time'].round(3),
            textposition='inside'
        ))
        fig_breakdown.add_trace(go.Bar(
            name='Overhead',
            x=time_breakdown['test_label'],
            y=time_breakdown['overhead_time'],
            text=time_breakdown['overhead_time'].round(3),
            textposition='inside'
        ))

        fig_breakdown.update_layout(
            barmode='stack',
            title='ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ êµ¬ì„± ë¹„êµ (í‰ê· )',
            xaxis_title='í…ŒìŠ¤íŠ¸',
            yaxis_title='ì‹œê°„ (ì´ˆ)',
            height=500,
            xaxis_tickangle=-45
        )
        st.plotly_chart(fig_breakdown, use_container_width=True)

    # Configuration comparison
    st.markdown("---")
    st.markdown("### âš™ï¸ ì„¤ì •ë³„ ì„±ëŠ¥ ë¹„êµ")

    config_comparison = df_metrics[['batch_size', 'num_connections', 'average_records_per_second', 'total_duration_seconds']].copy()
    config_comparison = config_comparison.groupby(['batch_size', 'num_connections']).agg({
        'average_records_per_second': 'mean',
        'total_duration_seconds': 'mean'
    }).reset_index()

    col1, col2 = st.columns(2)

    with col1:
        # Scatter plot: batch size vs throughput
        fig_scatter = px.scatter(
            df_metrics,
            x='batch_size',
            y='average_records_per_second',
            size='num_connections',
            color='cloud_provider',
            title='ë°°ì¹˜ í¬ê¸° vs ì²˜ë¦¬ëŸ‰',
            labels={
                'batch_size': 'ë°°ì¹˜ í¬ê¸°',
                'average_records_per_second': 'ì²˜ë¦¬ëŸ‰ (rec/s)',
                'num_connections': 'ì»¤ë„¥ì…˜ ìˆ˜',
                'cloud_provider': 'Provider'
            },
            hover_data=['instance_type']
        )
        fig_scatter.update_layout(height=400)
        st.plotly_chart(fig_scatter, use_container_width=True)

    with col2:
        # Scatter plot: connections vs throughput
        fig_scatter2 = px.scatter(
            df_metrics,
            x='num_connections',
            y='average_records_per_second',
            size='batch_size',
            color='cloud_provider',
            title='ì»¤ë„¥ì…˜ ìˆ˜ vs ì²˜ë¦¬ëŸ‰',
            labels={
                'num_connections': 'ì»¤ë„¥ì…˜ ìˆ˜',
                'average_records_per_second': 'ì²˜ë¦¬ëŸ‰ (rec/s)',
                'batch_size': 'ë°°ì¹˜ í¬ê¸°',
                'cloud_provider': 'Provider'
            },
            hover_data=['instance_type']
        )
        fig_scatter2.update_layout(height=400)
        st.plotly_chart(fig_scatter2, use_container_width=True)
