"""
Upload tab component for data upload functionality
"""
import streamlit as st
import pandas as pd
import json
import asyncio
from services.db_manager import DatabaseManager
from services.data_processor import DataProcessor


def run_performance_test(data, selected_clouds, chunk_size, config_loader):
    """Run performance test on uploaded data"""
    # Create progress container for real-time updates
    progress_container = st.empty()
    status_container = st.empty()

    with progress_container.container():
        st.info("ğŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        progress_bar = st.progress(0.0)

    with status_container.container():
        # Initialize database manager and data processor
        db_manager = DatabaseManager(config_loader)
        data_processor = DataProcessor(db_manager, chunk_size)

        # Run the test
        async def run_test():
            return await data_processor.process_all_clouds(data, selected_clouds)

        # Execute async function
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        processing_stats = loop.run_until_complete(run_test())
        loop.close()

        # Store results in session state
        st.session_state.processing_stats = processing_stats
        st.session_state.data_processor = data_processor

        # Create batch statistics from test results for consistency
        batch_stats = []
        for i, result in enumerate(data_processor.results):
            if result.success:
                batch_stats.append({
                    "batch_number": i + 1,
                    "table_name": f"sample_test_{result.cloud}",
                    "records_count": result.records_count,
                    "start_time": result.timestamp,
                    "end_time": result.timestamp + result.execution_time,
                    "total_duration_seconds": result.execution_time,
                    "execution_duration_seconds": result.execution_time,
                    "records_per_second": result.records_count / result.execution_time if result.execution_time > 0 else 0,
                    "cumulative_records": sum(r.records_count for r in data_processor.results[:i+1] if r.success)
                })
        st.session_state.current_batch_stats = batch_stats

    progress_bar.progress(1.0)
    st.success("âœ… í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'ì„±ëŠ¥ ë¹„êµ' íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.experimental_rerun()  # Enable auto reload for real-time updates


def render_upload_tab(chunk_size, selected_clouds, config_loader):
    """Render data upload tab"""
    st.header("ğŸ“¤ JSON ë°ì´í„° ì—…ë¡œë“œ")

    col1, col2 = st.columns([2, 1])

    with col1:
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_file = st.file_uploader(
            "JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['json'],
            help="í…ŒìŠ¤íŠ¸í•  JSON ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”"
        )

        if uploaded_file is not None:
            try:
                data = json.load(uploaded_file)
                if isinstance(data, dict):
                    data = [data]

                st.success(f"âœ… {len(data)}ê°œì˜ ë ˆì½”ë“œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

                # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                df_preview = pd.DataFrame(data[:5])  # Show first 5 records
                st.dataframe(df_preview)

                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë²„íŠ¼
                if st.button("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"):
                    if selected_clouds:
                        run_performance_test(data, selected_clouds, chunk_size, config_loader)
                    else:
                        st.error("í…ŒìŠ¤íŠ¸í•  í´ë¼ìš°ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”!")

            except json.JSONDecodeError:
                st.error("âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    with col2:
        st.subheader("ì˜ˆì‹œ ë°ì´í„°")
        if st.button("ğŸ“„ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©"):
            try:
                with open('data/sample_data.json', 'r', encoding='utf-8') as f:
                    sample_data = json.load(f)

                # ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.uploaded_data = sample_data
                st.success(f"âœ… {len(sample_data)}ê°œì˜ ìƒ˜í”Œ ë ˆì½”ë“œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

                # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                st.subheader("ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                df_sample = pd.DataFrame(sample_data[:3])
                st.dataframe(df_sample)

            except Exception as e:
                st.error(f"ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

        # ì„¸ì…˜ ìƒíƒœì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë²„íŠ¼ í‘œì‹œ
        if 'uploaded_data' in st.session_state and st.session_state.uploaded_data:
            data = st.session_state.uploaded_data
            st.write(f"í˜„ì¬ ë¡œë“œëœ ë°ì´í„°: {len(data)}ê°œ ë ˆì½”ë“œ")

            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë²„íŠ¼
            if st.button("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", key="sample_test_button"):
                if selected_clouds:
                    run_performance_test(data, selected_clouds, chunk_size, config_loader)
                else:
                    st.error("í…ŒìŠ¤íŠ¸í•  í´ë¼ìš°ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”!")