import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import asyncio
import json
import time
from datetime import datetime

# Import our custom modules
from config.config_loader import ConfigLoader
from services.db_manager import DatabaseManager
from services.data_processor import DataProcessor

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Cloud PostgreSQL Performance Tester",
    page_icon="ğŸš€",
    layout="wide"
)

# Initialize session state
if 'test_results' not in st.session_state:
    st.session_state.test_results = None
if 'processing_stats' not in st.session_state:
    st.session_state.processing_stats = None
if 'data_processor' not in st.session_state:
    st.session_state.data_processor = None

# ë©”ì¸ íƒ€ì´í‹€
st.title("ğŸš€ Cloud PostgreSQL Performance Tester")
st.markdown("í´ë¼ìš°ë“œ 3ì‚¬(GCP, Azure, AWS) PostgreSQL ì„±ëŠ¥ ë¹„êµ ë„êµ¬")

st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ í…ŒìŠ¤íŠ¸ ì„¤ì •")

# Load configuration
if 'config_loader' not in st.session_state:
    st.session_state.config_loader = ConfigLoader()

config_loader = st.session_state.config_loader

# í…ŒìŠ¤íŠ¸ ì„¤ì •
chunk_size = st.sidebar.slider("ì²­í¬ í¬ê¸°", 5, 100, 10, 10)
selected_clouds = st.sidebar.multiselect(
    "í…ŒìŠ¤íŠ¸í•  í´ë¼ìš°ë“œ",
    options=['gcp', 'azure', 'aws'],
    default=['gcp', 'azure', 'aws']
)

# Mock ëª¨ë“œ ì„¤ì •
mock_mode = st.sidebar.checkbox("Mock ëª¨ë“œ ì‚¬ìš©", value=True, help="ì‹¤ì œ DB ì—°ê²° ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸")

st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“Š ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")

if st.session_state.test_results is not None:
    if st.sidebar.button("CSVë¡œ ë‚´ë³´ë‚´ê¸°"):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = f"results/test_results_{timestamp}.csv"
        st.session_state.data_processor.export_results_to_csv(csv_path)
        st.sidebar.success(f"ê²°ê³¼ê°€ {csv_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

    if st.sidebar.button("JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = f"results/summary_{timestamp}.json"
        st.session_state.data_processor.export_summary_to_json(json_path)
        st.sidebar.success(f"ìš”ì•½ì´ {json_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ë©”ì¸ ì½˜í…ì¸ 
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“¤ ë°ì´í„° ì—…ë¡œë“œ", "ğŸ”„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜", "ğŸ“Š ì„±ëŠ¥ ë¹„êµ", "ğŸ“ˆ ìƒì„¸ ë¶„ì„", "âš™ï¸ ì„¤ì •"])

with tab1:
    st.header("ğŸ“¤ JSON ë°ì´í„° ì—…ë¡œë“œ")

    col1, col2 = st.columns([2, 1])

    with col1:
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_file = st.file_uploader(
            "JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['json'],
            help="í…ŒìŠ¤íŠ¸í•  JSON ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”"
        )

        if uploaded_file is not None:
            try:
                data = json.load(uploaded_file)
                if isinstance(data, dict):
                    data = [data]

                st.success(f"âœ… {len(data)}ê°œì˜ ë ˆì½”ë“œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

                # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                df_preview = pd.DataFrame(data[:5])  # Show first 5 records
                st.dataframe(df_preview)

                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë²„íŠ¼
                if st.button("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"):
                    if selected_clouds:
                        with st.spinner("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘..."):
                            # Initialize database manager and data processor
                            db_manager = DatabaseManager(config_loader)
                            data_processor = DataProcessor(db_manager, chunk_size)

                            # Run the test
                            async def run_test():
                                return await data_processor.process_all_clouds(data, selected_clouds)

                            # Execute async function
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            processing_stats = loop.run_until_complete(run_test())
                            loop.close()

                            # Store results in session state
                            st.session_state.processing_stats = processing_stats
                            st.session_state.data_processor = data_processor
                            st.session_state.test_results = data_processor.get_performance_summary()

                        st.success("âœ… í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'ì„±ëŠ¥ ë¹„êµ' íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                        # st.rerun()  # ì˜¤ë˜ëœ ë²„ì „ì—ì„œëŠ” ìë™ ë¦¬ë¡œë“œ ì•ˆí•¨
                    else:
                        st.error("í…ŒìŠ¤íŠ¸í•  í´ë¼ìš°ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”!")

            except json.JSONDecodeError:
                st.error("âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    with col2:
        st.subheader("ì˜ˆì‹œ ë°ì´í„°")
        if st.button("ğŸ“„ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©"):
            try:
                with open('data/sample_data.json', 'r', encoding='utf-8') as f:
                    sample_data = json.load(f)

                # ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.uploaded_data = sample_data
                st.success(f"âœ… {len(sample_data)}ê°œì˜ ìƒ˜í”Œ ë ˆì½”ë“œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

                # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                st.subheader("ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                df_sample = pd.DataFrame(sample_data[:3])
                st.dataframe(df_sample)

            except Exception as e:
                st.error(f"ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

        # ì„¸ì…˜ ìƒíƒœì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë²„íŠ¼ í‘œì‹œ
        if 'uploaded_data' in st.session_state and st.session_state.uploaded_data:
            data = st.session_state.uploaded_data
            st.write(f"í˜„ì¬ ë¡œë“œëœ ë°ì´í„°: {len(data)}ê°œ ë ˆì½”ë“œ")

            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë²„íŠ¼
            if st.button("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", key="sample_test_button"):
                if selected_clouds:
                    with st.spinner("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘..."):
                        # Initialize database manager and data processor
                        db_manager = DatabaseManager(config_loader)
                        data_processor = DataProcessor(db_manager, chunk_size)

                        # Run the test
                        async def run_test():
                            return await data_processor.process_all_clouds(data, selected_clouds)

                        # Execute async function
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        processing_stats = loop.run_until_complete(run_test())
                        loop.close()

                        # Store results in session state
                        st.session_state.processing_stats = processing_stats
                        st.session_state.data_processor = data_processor
                        st.session_state.test_results = data_processor.get_performance_summary()

                    st.success("âœ… í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'ì„±ëŠ¥ ë¹„êµ' íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    # st.experimental_rerun()  # ì˜¤ë˜ëœ ë²„ì „ì—ì„œëŠ” ìë™ ë¦¬ë¡œë“œ ì•ˆí•¨
                else:
                    st.error("í…ŒìŠ¤íŠ¸í•  í´ë¼ìš°ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”!")

with tab2:
    st.header("ğŸ”„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
    st.markdown("ê¸°ì¡´ ì…ì°° ë°ì´í„°ë¥¼ PostgreSQL í…Œì´ë¸”ì— ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.")

    # ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ import
    import os
    import json
    import psycopg2
    import psycopg2.extras
    from pathlib import Path
    from typing import Dict, List, Any, Optional
    import logging
    from datetime import datetime

    # Setup logging
    def setup_migration_logger():
        """Setup logger for migration process"""
        logger = logging.getLogger('migration')
        logger.setLevel(logging.INFO)

        # Clear existing handlers
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)

        # Create file handler
        log_filename = f"migration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_filename)
        file_handler.setLevel(logging.INFO)

        # Create formatter
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        return logger, log_filename

    class StreamlitDataMigrator:
        def __init__(self, logger=None):
            self.conn = None
            self.logger = logger or logging.getLogger(__name__)
            self.batch_performance_stats = []
            self.connect_to_db()

        def connect_to_db(self):
            """Connect to PostgreSQL database"""
            try:
                self.conn = psycopg2.connect(
                    host=os.getenv('GCP_DB_HOST'),
                    port=os.getenv('GCP_DB_PORT', 5432),
                    database=os.getenv('GCP_DB_NAME'),
                    user=os.getenv('GCP_DB_USER'),
                    password=os.getenv('GCP_DB_PASSWORD'),
                    sslmode='require'
                )
                self.logger.info("Successfully connected to PostgreSQL database")
                return True
            except Exception as e:
                error_msg = f"Database connection failed: {e}"
                self.logger.error(error_msg)
                st.error(error_msg)
                return False

        def get_table_name_from_filename(self, filename: str) -> Optional[str]:
            """Extract table name from filename"""
            if filename.startswith("BidPublicInfoService_BID_CNSTWK_"):
                return "bid_pblanclistinfo_cnstwk"
            elif filename.startswith("BidPublicInfoService_BID_SERVC_"):
                return "bid_pblanclistinfo_servc"
            elif filename.startswith("BidPublicInfoService_BID_THNG_"):
                return "bid_pblanclistinfo_thng"
            elif filename.startswith("BidPublicInfoService_BID_FRGCPT_"):
                return "bid_pblanclistinfo_frgcpt"
            elif filename.startswith("PubDataOpnStdService_ScsBidInfo_"):
                return "opn_std_scsbid_info"
            else:
                return None

        def get_table_columns(self, table_name: str) -> List[str]:
            """Get column names for a table (excluding auto-generated columns)"""
            try:
                cur = self.conn.cursor()
                cur.execute("""
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_name = %s
                    AND column_name NOT IN ('createdAt', 'updatedAt', 'id')
                    ORDER BY ordinal_position;
                """, (table_name,))

                columns = [row[0] for row in cur.fetchall()]
                cur.close()

                # Log columns found
                self.logger.info(f"Found {len(columns)} columns for {table_name}")
                self.logger.debug(f"Columns for {table_name}: {columns}")

                return columns
            except Exception as e:
                error_msg = f"Failed to get columns for {table_name}: {e}"
                self.logger.error(error_msg)
                st.error(error_msg)
                return []

        def prepare_record_data(self, record: Dict[str, Any], table_columns: List[str]) -> Dict[str, Any]:
            """Prepare record data to match table columns"""
            prepared_data = {}

            for column in table_columns:
                if column in record:
                    value = record[column]
                    if value is None:
                        prepared_data[column] = None
                    else:
                        prepared_data[column] = str(value) if not isinstance(value, str) else value
                else:
                    prepared_data[column] = None

            return prepared_data

        def insert_batch(self, table_name: str, records: List[Dict[str, Any]], batch_size: int = 100) -> int:
            """Insert records in batches with performance monitoring"""
            if not records:
                return 0

            table_columns = self.get_table_columns(table_name)
            if not table_columns:
                st.error(f"No columns found for table {table_name}")
                return 0

            total_inserted = 0
            insert_sql = ""

            try:
                cur = self.conn.cursor()

                for i in range(0, len(records), batch_size):
                    batch_start_time = time.time()
                    batch_number = i // batch_size + 1

                    batch = records[i:i + batch_size]
                    batch_data = []

                    for record in batch:
                        prepared_data = self.prepare_record_data(record, table_columns)
                        batch_data.append(prepared_data)

                    if batch_data:
                        # Add timestamp fields for all records
                        current_timestamp = 'NOW()'
                        for data in batch_data:
                            # These will be added as SQL functions, not as parameters
                            pass

                        # Quote column names to preserve case sensitivity
                        quoted_columns = ', '.join([f'"{col}"' for col in table_columns])
                        placeholders = ', '.join([f'%({col})s' for col in table_columns])

                        # Add timestamp columns
                        quoted_columns += ', "createdAt", "updatedAt"'
                        placeholders += ', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP'

                        insert_sql = f"INSERT INTO {table_name} ({quoted_columns}) VALUES ({placeholders})"

                        # Handle special case for opn_std_scsbid_info table
                        if table_name == 'opn_std_scsbid_info':
                            for j, data in enumerate(batch_data):
                                data['id'] = f"{data.get('bidNtceNo', '')}_{data.get('bidNtceOrd', '')}_{i+j+1}"

                            quoted_columns = '"id", ' + quoted_columns
                            placeholders = '%(id)s, ' + placeholders
                            insert_sql = f"INSERT INTO {table_name} ({quoted_columns}) VALUES ({placeholders})"

                        # Log the SQL for debugging (to file only)
                        self.logger.debug(f"SQL: {insert_sql}")
                        if i == 0:  # Log sample data for first batch
                            self.logger.debug(f"Sample data: {batch_data[0] if batch_data else 'No data'}")

                        # Execute batch with timing
                        exec_start_time = time.time()
                        cur.executemany(insert_sql, batch_data)
                        self.conn.commit()
                        exec_end_time = time.time()

                        total_inserted += len(batch_data)
                        batch_end_time = time.time()

                        # Calculate performance metrics
                        batch_duration = batch_end_time - batch_start_time
                        exec_duration = exec_end_time - exec_start_time
                        records_per_second = len(batch_data) / batch_duration if batch_duration > 0 else 0

                        # Store batch performance stats
                        batch_stat = {
                            "batch_number": batch_number,
                            "table_name": table_name,
                            "records_count": len(batch_data),
                            "start_time": datetime.fromtimestamp(batch_start_time),
                            "end_time": datetime.fromtimestamp(batch_end_time),
                            "total_duration_seconds": batch_duration,
                            "execution_duration_seconds": exec_duration,
                            "records_per_second": records_per_second,
                            "cumulative_records": total_inserted
                        }
                        self.batch_performance_stats.append(batch_stat)

                        # Log progress with performance info
                        self.logger.info(f"Batch {batch_number} for {table_name}: {len(batch_data)} records in {batch_duration:.3f}s ({records_per_second:.1f} rec/s)")

                cur.close()
                self.logger.info(f"Successfully inserted {total_inserted} records into {table_name}")

            except Exception as e:
                error_msg = f"Failed to insert batch into {table_name}: {str(e)}"
                self.logger.error(error_msg)
                self.logger.error(f"Failed SQL: {insert_sql}")
                if 'batch_data' in locals() and batch_data:
                    self.logger.error(f"Failed data sample: {batch_data[0]}")
                    self.logger.error(f"Available columns: {table_columns}")
                    self.logger.error(f"Data keys: {list(batch_data[0].keys()) if batch_data else 'No data'}")

                st.error(f"âŒ SQL Error in {table_name}: {str(e)}")
                self.conn.rollback()
                raise

            return total_inserted

        def process_file(self, file_path: Path, progress_bar) -> Dict[str, Any]:
            """Process a single JSON file"""
            filename = file_path.name
            table_name = self.get_table_name_from_filename(filename)

            if not table_name:
                self.logger.warning(f"Skipping {filename}: unknown file pattern")
                return {"filename": filename, "status": "skipped", "reason": "unknown file pattern"}

            try:
                self.logger.info(f"Processing {filename} -> {table_name}")

                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if not isinstance(data, list):
                    self.logger.error(f"Invalid data format in {filename}: expected list, got {type(data)}")
                    return {"filename": filename, "status": "error", "reason": "invalid data format"}

                self.logger.info(f"Loaded {len(data)} records from {filename}")
                inserted_count = self.insert_batch(table_name, data)
                progress_bar.progress(1.0)

                result = {
                    "filename": filename,
                    "table": table_name,
                    "status": "success",
                    "records_processed": len(data),
                    "records_inserted": inserted_count
                }

                self.logger.info(f"Completed {filename}: {inserted_count} records inserted")
                return result

            except Exception as e:
                error_msg = f"Failed to process {filename}: {str(e)}"
                self.logger.error(error_msg)
                return {
                    "filename": filename,
                    "table": table_name,
                    "status": "error",
                    "reason": str(e)
                }

        def get_table_counts(self) -> Dict[str, int]:
            """Get record counts for all tables"""
            tables = [
                'bid_pblanclistinfo_cnstwk',
                'bid_pblanclistinfo_frgcpt',
                'bid_pblanclistinfo_servc',
                'bid_pblanclistinfo_thng',
                'opn_std_scsbid_info'
            ]

            counts = {}
            try:
                cur = self.conn.cursor()
                for table in tables:
                    cur.execute(f'SELECT COUNT(*) FROM {table}')
                    counts[table] = cur.fetchone()[0]
                cur.close()
            except Exception as e:
                st.error(f"Failed to get table counts: {e}")

            return counts

        def get_batch_performance_stats(self) -> List[Dict[str, Any]]:
            """Get batch performance statistics"""
            return self.batch_performance_stats

        def get_performance_summary(self) -> Dict[str, Any]:
            """Get performance summary statistics"""
            if not self.batch_performance_stats:
                return {}

            total_batches = len(self.batch_performance_stats)
            total_records = sum(stat['records_count'] for stat in self.batch_performance_stats)
            total_duration = sum(stat['total_duration_seconds'] for stat in self.batch_performance_stats)

            avg_batch_time = total_duration / total_batches if total_batches > 0 else 0
            avg_records_per_second = total_records / total_duration if total_duration > 0 else 0

            # Group by table
            table_stats = {}
            for stat in self.batch_performance_stats:
                table = stat['table_name']
                if table not in table_stats:
                    table_stats[table] = {
                        'batches': 0,
                        'records': 0,
                        'duration': 0,
                        'avg_rps': 0
                    }
                table_stats[table]['batches'] += 1
                table_stats[table]['records'] += stat['records_count']
                table_stats[table]['duration'] += stat['total_duration_seconds']

            # Calculate averages for each table
            for table, stats in table_stats.items():
                if stats['duration'] > 0:
                    stats['avg_rps'] = stats['records'] / stats['duration']

            return {
                'total_batches': total_batches,
                'total_records': total_records,
                'total_duration_seconds': total_duration,
                'average_batch_time_seconds': avg_batch_time,
                'average_records_per_second': avg_records_per_second,
                'table_statistics': table_stats
            }

        def close(self):
            """Close database connection"""
            if self.conn:
                self.conn.close()

    # ë§ˆì´ê·¸ë ˆì´ì…˜ UI
    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("ğŸ“ ë°ì´í„° íŒŒì¼ í˜„í™©")

        # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
        data_path = Path("data")
        if data_path.exists():
            json_files = [f for f in data_path.glob("*.json") if f.name != "sample_data.json"]

            if json_files:
                st.success(f"âœ… {len(json_files)}ê°œì˜ ë°ì´í„° íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

                # íŒŒì¼ ëª©ë¡ í‘œì‹œ
                file_info = []
                total_size = 0

                for file_path in sorted(json_files):
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    total_size += size_mb

                    table_name = "Unknown"
                    if file_path.name.startswith("BidPublicInfoService_BID_CNSTWK_"):
                        table_name = "bid_pblanclistinfo_cnstwk"
                    elif file_path.name.startswith("BidPublicInfoService_BID_SERVC_"):
                        table_name = "bid_pblanclistinfo_servc"
                    elif file_path.name.startswith("BidPublicInfoService_BID_THNG_"):
                        table_name = "bid_pblanclistinfo_thng"
                    elif file_path.name.startswith("BidPublicInfoService_BID_FRGCPT_"):
                        table_name = "bid_pblanclistinfo_frgcpt"
                    elif file_path.name.startswith("PubDataOpnStdService_ScsBidInfo_"):
                        table_name = "opn_std_scsbid_info"

                    file_info.append({
                        "íŒŒì¼ëª…": file_path.name,
                        "í¬ê¸° (MB)": f"{size_mb:.2f}",
                        "ëŒ€ìƒ í…Œì´ë¸”": table_name
                    })

                df_files = pd.DataFrame(file_info)
                st.dataframe(df_files, use_container_width=True)

                st.info(f"ì´ ë°ì´í„° í¬ê¸°: {total_size:.2f} MB")

                # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ë²„íŠ¼
                if st.button("ğŸš€ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰", type="primary"):
                    # Setup logger
                    logger, log_filename = setup_migration_logger()
                    st.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: `{log_filename}`")

                    migrator = StreamlitDataMigrator(logger)

                    if migrator.conn:
                        # ì´ˆê¸° í…Œì´ë¸” ì¹´ìš´íŠ¸
                        st.subheader("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰ìƒí™©")
                        initial_counts = migrator.get_table_counts()

                        st.write("**ì´ˆê¸° í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜:**")
                        for table, count in initial_counts.items():
                            st.write(f"  - {table}: {count:,} records")

                        # ì§„í–‰ìƒí™© ì¶”ì 
                        progress_container = st.container()
                        results = []

                        for i, file_path in enumerate(sorted(json_files)):
                            with progress_container:
                                st.write(f"ì²˜ë¦¬ ì¤‘: {file_path.name}")
                                file_progress = st.progress(0)

                                result = migrator.process_file(file_path, file_progress)
                                results.append(result)

                                if result["status"] == "success":
                                    st.success(f"âœ… {result['filename']}: {result['records_inserted']:,} ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ")
                                else:
                                    st.error(f"âŒ {result['filename']}: {result.get('reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

                        # ìµœì¢… ê²°ê³¼
                        st.subheader("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")

                        successful = [r for r in results if r["status"] == "success"]
                        failed = [r for r in results if r["status"] == "error"]
                        skipped = [r for r in results if r["status"] == "skipped"]

                        total_records = sum(r.get("records_inserted", 0) for r in successful)

                        col_a, col_b, col_c, col_d = st.columns(4)
                        with col_a:
                            st.metric("ì´ íŒŒì¼", len(results))
                        with col_b:
                            st.metric("ì„±ê³µ", len(successful))
                        with col_c:
                            st.metric("ì‹¤íŒ¨", len(failed))
                        with col_d:
                            st.metric("ì´ ë ˆì½”ë“œ", f"{total_records:,}")

                        # ìµœì¢… í…Œì´ë¸” ì¹´ìš´íŠ¸
                        final_counts = migrator.get_table_counts()

                        st.write("**ìµœì¢… í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜:**")
                        for table, count in final_counts.items():
                            initial = initial_counts.get(table, 0)
                            added = count - initial
                            st.write(f"  - {table}: {count:,} records (+{added:,})")

                        # ë°°ì¹˜ ì„±ëŠ¥ ë¶„ì„ í‘œì‹œ
                        batch_stats = migrator.get_batch_performance_stats()
                        performance_summary = migrator.get_performance_summary()

                        if batch_stats and performance_summary:
                            st.markdown("---")
                            st.subheader("ğŸ“ˆ ë°°ì¹˜ë³„ ì„±ëŠ¥ ë¶„ì„")

                            # ì„±ëŠ¥ ìš”ì•½ ë©”íŠ¸ë¦­
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("ì´ ë°°ì¹˜ ìˆ˜", performance_summary['total_batches'])
                            with col2:
                                st.metric("í‰ê·  ë°°ì¹˜ ì‹œê°„", f"{performance_summary['average_batch_time_seconds']:.3f}ì´ˆ")
                            with col3:
                                st.metric("í‰ê·  ì²˜ë¦¬ëŸ‰", f"{performance_summary['average_records_per_second']:.1f} rec/s")
                            with col4:
                                st.metric("ì´ ì²˜ë¦¬ ì‹œê°„", f"{performance_summary['total_duration_seconds']:.1f}ì´ˆ")

                            # ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹œê°„ ì°¨íŠ¸
                            df_batch_stats = pd.DataFrame(batch_stats)

                            if not df_batch_stats.empty:
                                # ì²˜ë¦¬ ì‹œê°„ ì¶”ì´ ì°¨íŠ¸
                                fig_timeline = px.line(
                                    df_batch_stats,
                                    x='batch_number',
                                    y='total_duration_seconds',
                                    color='table_name',
                                    title='ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹œê°„ ì¶”ì´',
                                    labels={
                                        'batch_number': 'ë°°ì¹˜ ë²ˆí˜¸',
                                        'total_duration_seconds': 'ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)',
                                        'table_name': 'í…Œì´ë¸”'
                                    }
                                )
                                fig_timeline.update_layout(height=400)
                                st.plotly_chart(fig_timeline, use_container_width=True)

                                # ì²˜ë¦¬ëŸ‰ ì¶”ì´ ì°¨íŠ¸
                                fig_throughput = px.line(
                                    df_batch_stats,
                                    x='batch_number',
                                    y='records_per_second',
                                    color='table_name',
                                    title='ë°°ì¹˜ë³„ ì²˜ë¦¬ëŸ‰ ì¶”ì´',
                                    labels={
                                        'batch_number': 'ë°°ì¹˜ ë²ˆí˜¸',
                                        'records_per_second': 'ì²˜ë¦¬ëŸ‰ (records/sec)',
                                        'table_name': 'í…Œì´ë¸”'
                                    }
                                )
                                fig_throughput.update_layout(height=400)
                                st.plotly_chart(fig_throughput, use_container_width=True)

                                # í…Œì´ë¸”ë³„ ì„±ëŠ¥ ìš”ì•½
                                if 'table_statistics' in performance_summary:
                                    st.subheader("í…Œì´ë¸”ë³„ ì„±ëŠ¥ ìš”ì•½")
                                    table_summary_data = []
                                    for table, stats in performance_summary['table_statistics'].items():
                                        table_summary_data.append({
                                            'í…Œì´ë¸”': table,
                                            'ë°°ì¹˜ ìˆ˜': stats['batches'],
                                            'ì´ ë ˆì½”ë“œ': f"{stats['records']:,}",
                                            'ì´ ì‹œê°„ (ì´ˆ)': f"{stats['duration']:.2f}",
                                            'í‰ê·  ì²˜ë¦¬ëŸ‰ (rec/s)': f"{stats['avg_rps']:.1f}"
                                        })

                                    if table_summary_data:
                                        df_table_summary = pd.DataFrame(table_summary_data)
                                        st.dataframe(df_table_summary, use_container_width=True)

                        migrator.close()

            else:
                st.warning("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error("data ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    with col2:
        st.subheader("ğŸ“‹ í˜„ì¬ í…Œì´ë¸” ìƒíƒœ")

        if st.button("ğŸ”„ í…Œì´ë¸” ìƒíƒœ ìƒˆë¡œê³ ì¹¨"):
            try:
                # Use a simple logger for refresh operation
                refresh_logger = logging.getLogger('refresh')
                migrator = StreamlitDataMigrator(refresh_logger)
                if migrator.conn:
                    counts = migrator.get_table_counts()

                    st.write("**í˜„ì¬ ë ˆì½”ë“œ ìˆ˜:**")
                    for table, count in counts.items():
                        st.write(f"  - {table}: {count:,}")

                    migrator.close()
            except Exception as e:
                st.error(f"í…Œì´ë¸” ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")

with tab3:
    st.header("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")

    if st.session_state.test_results is not None:
        results = st.session_state.test_results
        stats = st.session_state.processing_stats

        # ì „ì²´ í†µê³„
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("ì´ ë ˆì½”ë“œ ìˆ˜", stats.total_records)
        with col2:
            st.metric("ì´ ì²­í¬ ìˆ˜", stats.total_chunks)
        with col3:
            st.metric("ì²˜ë¦¬ ì‹œê°„", f"{stats.processing_time:.2f}ì´ˆ")
        with col4:
            success_rate = (stats.success_count / (stats.success_count + stats.failure_count)) * 100
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")

        st.markdown("---")

        # í´ë¼ìš°ë“œë³„ ì„±ëŠ¥ ë¹„êµ
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("í‰ê·  ì‹¤í–‰ ì‹œê°„ ë¹„êµ")

            cloud_names = []
            avg_times = []

            for cloud, data in results.items():
                if data['successful_operations'] > 0:
                    cloud_names.append(cloud.upper())
                    avg_times.append(data['average_execution_time'])

            if cloud_names:
                fig_bar = px.bar(
                    x=cloud_names,
                    y=avg_times,
                    labels={'x': 'Cloud Provider', 'y': 'Average Execution Time (seconds)'},
                    color=avg_times,
                    color_continuous_scale='Viridis'
                )
                fig_bar.update_layout(showlegend=False)
                st.plotly_chart(fig_bar, use_container_width=True)

        with col2:
            st.subheader("ì²˜ë¦¬ëŸ‰ ë¹„êµ (records/sec)")

            cloud_names = []
            throughput = []

            for cloud, data in results.items():
                if data['successful_operations'] > 0:
                    cloud_names.append(cloud.upper())
                    throughput.append(data['records_per_second'])

            if cloud_names:
                fig_throughput = px.bar(
                    x=cloud_names,
                    y=throughput,
                    labels={'x': 'Cloud Provider', 'y': 'Records per Second'},
                    color=throughput,
                    color_continuous_scale='Plasma'
                )
                fig_throughput.update_layout(showlegend=False)
                st.plotly_chart(fig_throughput, use_container_width=True)

        # ìƒì„¸ í†µê³„ í…Œì´ë¸”
        st.subheader("ìƒì„¸ ì„±ëŠ¥ í†µê³„")

        summary_data = []
        for cloud, data in results.items():
            summary_data.append({
                'Cloud': cloud.upper(),
                'ì´ ì‘ì—…': data['total_operations'],
                'ì„±ê³µ': data['successful_operations'],
                'ì‹¤íŒ¨': data['failed_operations'],
                'ì„±ê³µë¥  (%)': f"{data['success_rate']:.1f}",
                'í‰ê·  ì‹œê°„ (ì´ˆ)': f"{data['average_execution_time']:.4f}",
                'ìµœì†Œ ì‹œê°„ (ì´ˆ)': f"{data['min_execution_time']:.4f}",
                'ìµœëŒ€ ì‹œê°„ (ì´ˆ)': f"{data['max_execution_time']:.4f}",
                'ì²˜ë¦¬ëŸ‰ (records/sec)': f"{data['records_per_second']:.2f}"
            })

        df_summary = pd.DataFrame(summary_data)
        st.dataframe(df_summary, use_container_width=True)

    else:
        st.info("í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")

with tab4:
    st.header("ğŸ“ˆ ìƒì„¸ ë¶„ì„")

    if st.session_state.data_processor is not None:
        processor = st.session_state.data_processor

        # ì‹œê°„ë³„ ì„±ëŠ¥ ë¶„ì„
        st.subheader("ì²­í¬ë³„ ì‹¤í–‰ ì‹œê°„ ë¶„ì„")

        results_by_cloud = processor.get_results_by_cloud()

        fig_timeline = go.Figure()

        for cloud, cloud_results in results_by_cloud.items():
            successful_results = [r for r in cloud_results if r.success]
            if successful_results:
                chunk_ids = [r.chunk_id for r in successful_results]
                exec_times = [r.execution_time for r in successful_results]

                fig_timeline.add_trace(go.Scatter(
                    x=chunk_ids,
                    y=exec_times,
                    mode='lines+markers',
                    name=cloud.upper(),
                    line=dict(width=2),
                    marker=dict(size=6)
                ))

        fig_timeline.update_layout(
            xaxis_title="Chunk ID",
            yaxis_title="Execution Time (seconds)",
            hovermode='x unified'
        )
        st.plotly_chart(fig_timeline, use_container_width=True)

        # ì‹¤í–‰ ì‹œê°„ ë¶„í¬
        st.subheader("ì‹¤í–‰ ì‹œê°„ ë¶„í¬")

        col1, col2 = st.columns(2)

        with col1:
            # Box plot
            fig_box = go.Figure()

            for cloud, cloud_results in results_by_cloud.items():
                successful_results = [r for r in cloud_results if r.success]
                if successful_results:
                    exec_times = [r.execution_time for r in successful_results]
                    fig_box.add_trace(go.Box(
                        y=exec_times,
                        name=cloud.upper(),
                        boxpoints='all',
                        jitter=0.3,
                        pointpos=-1.8
                    ))

            fig_box.update_layout(
                yaxis_title="Execution Time (seconds)",
                showlegend=False
            )
            st.plotly_chart(fig_box, use_container_width=True)

        with col2:
            # Histogram
            selected_cloud_detail = st.selectbox(
                "ë¶„ì„í•  í´ë¼ìš°ë“œ ì„ íƒ",
                options=list(results_by_cloud.keys()),
                format_func=lambda x: x.upper()
            )

            if selected_cloud_detail in results_by_cloud:
                cloud_results = results_by_cloud[selected_cloud_detail]
                successful_results = [r for r in cloud_results if r.success]

                if successful_results:
                    exec_times = [r.execution_time for r in successful_results]

                    fig_hist = px.histogram(
                        x=exec_times,
                        nbins=20,
                        labels={'x': 'Execution Time (seconds)', 'y': 'Count'},
                        title=f"{selected_cloud_detail.upper()} ì‹¤í–‰ ì‹œê°„ íˆìŠ¤í† ê·¸ë¨"
                    )
                    st.plotly_chart(fig_hist, use_container_width=True)

    else:
        st.info("í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ ìƒì„¸ ë¶„ì„ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")

with tab5:
    st.header("âš™ï¸ ì„¤ì •")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •")

        # í˜„ì¬ ì„¤ì • í‘œì‹œ
        db_config = config_loader.load_database_config()

        for cloud, config in db_config.get('clouds', {}).items():
            with st.expander(f"{cloud.upper()} ì„¤ì •"):
                st.code(f"""
Host: {config.get('host', 'Not set')}
Port: {config.get('port', 5432)}
Database: {config.get('database', 'Not set')}
User: {config.get('user', 'Not set')}
SSL Mode: {config.get('ssl_mode', 'require')}
                """)

    with col2:
        st.subheader("ìŠ¤í‚¤ë§ˆ ì„¤ì •")

        # í˜„ì¬ ìŠ¤í‚¤ë§ˆ í‘œì‹œ
        schema_config = config_loader.load_schema()

        st.code(f"Table Name: {schema_config.get('table_name', 'test_data')}")

        with st.expander("í•„ë“œ ì •ì˜"):
            for field_name, field_config in schema_config.get('fields', {}).items():
                st.write(f"**{field_name}**: {field_config.get('type', 'Unknown')} - {field_config.get('description', 'No description')}")

# í‘¸í„°
st.markdown("---")
st.markdown("ğŸš€ **Cloud PostgreSQL Performance Tester** - í´ë¼ìš°ë“œ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë„êµ¬")